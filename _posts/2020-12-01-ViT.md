---
layout: single
title:  "[Review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
date:   2020-12-01
categories: CV
toc: true
---
[[논문]](https://arxiv.org/pdf/2010.11929.pdf)  

장안의 화제가 되었던 논문입니다. 최근 DETR 논문을 굉장히 흥미롭게 읽어서, Transformer를 CV에 활용하는 다른 사례에 대해서도 관심이 있었습니다. 그러다 우연히 Image Classification Benchmark를 보는데(최신 backbone들이 궁금해서..) ViT가 state of the art로 올라와 있는 것을 보고 **"이건 꼭 읽어봐야 해!"** 라는 생각이 들어 바로 리뷰해보게 되었습니다.

![vit_sota](/assets/images/201201/vit_sota.png)
*ImageNet Benchmark에서 SOTA에 올라있는 ViT*
<br />
<br />

# 들어가기 전
- 이름이 왜 ViT인가요?
  - Vision Transformer의 약자입니다.
- 핵심 아이디어는 뭔가요?
  - 이미지를 작은 patch들로 쪼개어 각각을 '단어'로, 전체 이미지를 '문장'으로 생각하여 transformer로 학습을 시도합니다.
- 필요한 배경 지식이 있나요?
  - 다음 두 가지 논문을 참고하면서 읽으시면 도움이 됩니다!
    - Attention is all you need
    - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding


<br />
<br />

# Introduction
- [**Transformer**](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)의 등장 이후로, NLP 분야에서는 전례없던 크기의 모델을 pretraining -> fine-tuning의 방식으로 학습할 수 있게 되었습니다. 심지어는 데이터셋과 모델이 커지면 커질수록 성능이 계속 좋아져 Saturate 되기까지도 많은 시간이 걸릴 것으로 보입니다. 
- 비전에서도 Transformer에 쓰인 Self-attention을 적용하기 위한 여러 노력들이 있었으나, scalability가 부족하거나, 성능이 뛰어나지 않은 경우가 대다수였습니다.
- 본 논문에서는 Transformer를 직접적으로 이미지에 적용하는 것을 시도합니다. 이미지를 'patch' 단위로 쪼개어 임베딩을 만들고, NLP의 Token과 같이 처리합니다.
- 결과는 어땠냐구요? 작은 데이터셋에서 학습시켰을 때는 CNN모델에 비해 부족한 성능을 보였지만, 큰 데이터셋(JFT-300M, ImageNet-21k)에서 학습시켰을 때는 **resnet based 모델을 뛰어넘는 성능**을 보여줬습니다. 이는 데이터가 클수록, CNN에 비해 robust한 inductive bias를 가진 Transformer가 더욱 좋은 generalization 성능을 낸다는 것을 보여줍니다.
<br />
<br />

# Related Work
- Transformer가 2017년에 등장한 이후, Transformer-based 모델들(BERT, GPT 계열)은 NLP 태스크에서 state-of-the-art를 달성해왔습니다.
- 이러한 Transformer의 성공으로, self-attention mechanism을 비전에 도입하려는 여러 시도들이 있었습니다. Naive한 pixel by pixel global attention은 너무 많은 computational cost가 필요하므로, 이를 근사하기 위해 다음과 같은 몇 가지 방법들이 제안되었습니다.
  - local pixel들에만 self-attention을 적용
  - 블록의 크기를 바꾸어가며 블록 단위의 self-attention을 적용
  - etc..
- 또한, 기존 CNN-based 모델과 결합하여 사용하려는 시도도 있었습니다. (ex. DETR)
- 본 연구에서는 patch 단위에서 self-attention을 적용한 ViT를 제시하고, 최근 흐름에 발맞추어 ImageNet보다 더 큰 규모의 데이터셋들을 사용하여 학습하고, 평가합니다. 
<br />
<br />

# Model
![model_structure](/assets/images/201201/model_structure.png)
*ViT의 구조*
## Vision Transformer
  위 그림이 Vision Transformer(ViT)의 구조입니다. input이 image patch라는 것을 빼면 *Attention is all you need*에 나오는 Transformer Encoder와 크게 다르지 않습니다. input을 가공하는 과정 외에는 사실상 똑같다고 봐야하는데, 그 과정을 순서대로 정리하면 아래와 같습니다.
- 이미지 데이터 프로세싱
  - HxWxC 이미지 데이터를 Nx(P^2*C)로 바꾸어 줍니다. P는 한 patch의 한 변의 픽셀 개수이고, N = HW/(P^2) 입니다.
  - 이제 N개의 P^2*C dimension을 가지는 벡터를 얻었습니다. 이 벡터들을 Dense layer를 통과시켜 D차원으로 mapping 시킵니다.
  - 이제 우리는 **HxWxC** 이미지 데이터를 **NxD** 데이터로 바꾸었습니다. 이는 NLP에서 각 문장 속 단어 임베딩을 나열해놓은 것과 유사합니다(단어 N개를 가지는 문장).
- class 토큰 추가
  - 논문에서 해결하고자 하는 태스크는 image classification이므로 맨 앞에 **[class]** 토큰을 추가해줍니다. 이는 BERT의 [cls] 토큰과 같은 역할로, 토큰은 학습가능한 D차원 임베딩을 가지며, encoder의 맨마지막 레이어의 출력값 중 [class] 토큰에 classification head를 연결해 태스크를 해결합니다(자세한 설명은 BERT 논문 참조).
  - 그러면 [class] 토큰까지 포함해 (N+1)xD 데이터를 얻었습니다!
- position embedding
  - encoder에 처음 input으로 들어갈 때, 각 벡터에 learnable한 **1D positional** embedding이 더해집니다.
  - 2D position embedding은 실험 결과 좋은 결과가 나오지 않았다고 합니다.
<br />

## Hybrid Architecture
위 모델은 CNN 구조를 완전히 배제한 모델인데요, 논문에서는 CNN-based 모델과 결합한 Hybrid 모델도 함께 제시했습니다. Hybrid Architecture에서는 CNN-based 모델을 통과시켜 나온 feature map을 그냥 flatten하여 사용합니다. 각 patch는 feature map의 한 pixel이 되고, dimension D는 마지막 Convolutional layer의 채널 수가 됩니다.
<br />

## Fine-Tuning And Higher Resolution
모델의 Fine-tuning은 일반적인 방식과 동일하게, 학습 후 classification head를 제거하고, zero-initialized head를 붙여서 학습시키는 방법으로 이루어집니다. 이때, pretraining보다 fine-tuning에서 더 높은 resolution의 이미지로 학습시키면 더 좋은 성능을 냄이 알려져 있다고 하는데요. patch size는 고정돼 있기 때문에 resolution을 높이면 sequence length가 커지게 되겠죠. ViT는 sequence Length가 달라져도 dynamic하게 처리할 수 있는 모델이지만, position embedding에는 문제가 생깁니다. 이 문제를 해결하기 위해서 논문에서는 2D interpolation을 통해 적절한 position embedding을 얻어 사용하였고, 이것이 ViT에서 사용된 2D structure와 관련한 사실상 유일한 inductive bias라고 밝혔습니다.
<br />
<br />

# Experiments
## Datasets
- ImageNet-21K와 JFT에 pretrain을 한 후 ImageNet, CIFAR, Oxford-IIIT Pets 등 다양한 데이터셋에서 평가를 진행했습니다.
<br />

## Model Variants
- 다양한 크기의 ViT를 사용했는데, ViT-Base/Large/Huge 총 3가지의 ViT를 활용했습니다. 또한, patch size에 따라서 모델을 다르게 표기하였습니다. 예를 들어 ViT-L/16 은 ViT Large 모델이고, patch size가 16 x 16인 모델입니다. 아래는 각 모델의 속성입니다.  
![model_variants](/assets/images/201201/model_variants.png)
- CNN-based 모델의 대표로는 BiT(Big Transfer), Noisy Student 모델을 사용했습니다. Noisy Student 모델은 ImageNet에서, BiT는 다른 여러 데이터셋에서 SOTA를 달성한 모델이라고 합니다.

## Metric
- 대부분 fine-tuning accuracy를 metric으로 잡았지만, fine-tuning은 computationally expensive하므로 encoder를 freeze하고 head에서 regularized linear regression을 푸는 few-shot accuracy를 기록하기도 했습니다.
<br />

## State-of-the-art 모델들과의 비교
![sota_models](/assets/images/201201/sota_models.png)
<br />

- JFT에서 pretrained된 ViT-L/16 모델이 같은 데이터셋을 사용한 BiT-L과 비등하거나 더 좋은 성능을 보여주었으며, ViT-H/14 모델은 모든 태스크에서 우월한 성능을 보여주었습니다.
- TPUv3-core-days는 사용된 TPU코어의 수 x 걸린 날의 수를 나타낸 것으로, I21K에 pretrained된 ViT-L/16은 8개의 TPU로 한 달이면 학습시킬 수 있을만큼 기존 모델에 비해 적은 computational cost를 요구합니다.
<br />

## Pre-training Data Requirements
![pretraining_dataset](/assets/images/201201/pretraining_dataset.png)
<br />

왼쪽은 Pre-training dataset을 달리했을 경우, 나타나는 ImageNet dataset에서의 accuracy를 여러 모델에 대해 plot한 것입니다.
- ImageNet에 pretrain시켰을 때는, BiT가 가장 좋은 성능을 보여주었고, ViT-L모델은 ViT-B모델보다도 안좋은 성능을 보여주었습니다.
- I21K에 pretrain시키자, ViT-L모델이 ViT-B모델보다 높은 accuracy를 기록했고, BiT와도 비슷한 수준까지 올라왔습니다.
- 마지막으로, JFT에 pretrain시키자 ViT-H모델은 BiT보다 더 높은 accuracy를 기록하였습니다.
<br />

오른쪽은 JFT에서 pretraining sample의 수를 달리하며 기록한 성능입니다. 이때, computational cost를 줄이기 위해 fine-tuning 대신 few-shot learning을 했다고 합니다.
- 왼쪽 그래프와 비슷하게, pretraining sample이 적을수록 ViT보다는 Resnet-based 모델이 좋은 성능을 보여줍니다.
- 이는 relevance를 찾는 방법을 학습하는 ViT의 inductive bias가 작은 데이터셋에서는 overfit되어 general한 feature를 뽑아내지 못하는 반면, 데이터셋이 커질수록 더욱 general한 relevance를 학습하는 것으로 해석할 수 있습니다.
- 반면 Resnet-based model은 일반적인 이미지 데이터가 가지는 locality, translation equivalance 등의 inductive bias를 반영하고 있기에 적은 양의 데이터셋에서는 더 우수하지만, 데이터셋이 커질수록 ViT가 더 우수한 성능을 보입니다.
<br />
<br />

## Scaling Study
![pretraining_computation](/assets/images/201201/pretrain_computation.png)
pretraining에 사용된 computational cost(epoch, 모델 크기 등을 반영한)에 따른 모델 accuracy입니다.
- 모든 구간에서 ViT가 BiT보다 우수한 성능을 보여주었습니다.
- Hybrid 모델의 경우, 데이터셋의 크기가 작을 경우 ViT보다 더 좋은 성능을 보여주었지만, 데이터셋의 크기가 커지면서 거의 똑같은 accuracy를 기록했습니다. convolutional layer를 통해 추출된 feature가 ViT를 보완할 것이라고 생각했지만 데이터셋이 충분히 클 경우에 self-attention만으로도 충분히 convolution연산을 대체할 수 있는 것으로 보입니다.
- ViT는 saturate되지 않았으므로 더 큰 데이터셋에 더 오래 동안 학습시키면, 더 좋은 성능을 기대할 수도 있을 것입니다.
<br />
<br />

## Inspecting Vision Transformer
![inspection](/assets/images/201201/inspection.png)
<br />
<br />
- 왼쪽 그림은 patch의 vector representation을 얻을 때 사용하는 filter중 맨 앞 28개를 시각화 한 것입니다. 웬 filter냐고요? 각 patch를 dense layer를 통과시켜 dimension을 줄이는 작업을 convolution layer를 이용해 구현했기 때문입니다(논문에서는 언급이 일절 없습니다(...)). low dimension의 fine-structure를 가지는 representation의 basis function들을 닮았다는데... 대략 가로/세로로 줄무늬가 생기고, 가운데에 가중치가 생겨서 그런 말을 하는 것 같습니다. 자세하게 이해하신 분께서 가르침을 주시면 좋겠네요..
- 가운데 그림은 학습된 position embedding의 유사도 행렬을 시각화한 것입니다. 같은 row/column의 patch들에 가중치가 높게 학습이 된 것을 확인할 수 있습니다. 2D structure를 잘 반영하도록 학습이 된 것이죠.
- 오른쪽 그림은 Multi-Head Self-Attention에서 각 head들의 mean attention distance를 나타낸 것입니다. 이 "attention distance"는 CNN에서의 "receptive field"와 유사하게 해석할 수 있습니다.
  - 낮은 layer에서도 global한(큰 attention distance를 가지는) attention이 사용되고 있는 것을 확인할 수 있습니다.
  - hybrid model은 highly local한(작은 attention distance를 가지는) head가 ViT에 비해 비교적 적게 나타난다고 합니다. 이는 이러한 head들이 기존의 convolution연산을 대체하고 있음을 보여줍니다.
  - encoder depth가 커질수록, 평균 attention distance가 커집니다. 이는 단순히 거리에 의한 것이 아닌 "content-based" attention이 일어나고 있음을 시사합니다.
<br />
<br />

## Self-Supervision
BERT가 성공한 것은 Transformer의 뛰어난 scalability 때문도 있겠지만, MLM과 같은 self-supervised learning 덕분이기도 합니다. 그래서 본 논문에서도 patch를 예측하는 masked patch prediction 방법으로 self-supervised learning을 수행하였는데, 그 결과는 79.9%로, naive한 학습보다는 좋지만 pre-training 보다는 낮은 성능이라고 합니다. 후속 연구에서 다루어지면 재밌는 주제가 될 것 같습니다.
<br />
<br />

# Conclusion
본 논문에서는 computationally cheap하고, 뛰어난 성능을 가지는 ViT(Vision Transformer)라는 모델을 제시했습니다. ViT는 image를 여러 개의 patch로 나누어 Transformer에 넣는, 이미지에 대한 inductive bias를 최소화한 모델입니다. 앞으로 ViT를 다른 태스크에 적용하는 것과 self-supervised learning이나 scaling을 통해 성능을 끌어올리는 것이 과제로 남았습니다.

<br />
<br />

# 마치며
비전 모델들의 성능이 점점 saturated되면서 모델 구조가 점점 복잡해지는 와중에 가뭄의 단비 같은 논문인 것 같습니다. 또, 가장 기본적인 모델이다 보니 발전 가능성도 많아 보입니다(patch를 자르는 과정에서 손실되는 정보 및 발생하는 모델링 능력의 병목을 최소화 하는 방법에 대해 고민해보면 좋을 것 같네요)! Transformer의 relevance 모델링 능력과 robust한 inductive bias덕분에 앞으로도 더 다양한 분야로의 적용을 기대해보아도 좋을 것 같네요. 진짜 human-like robot이 만들어지고, 거기에 딥러닝 모델들이 탑재된다면 분명 transformer-based가 아닐까 예상해봅니다.