---
layout: single
title:  "[CV] Review: MLP-Mixer: An all-MLP Architecture for Vision"
date:   2021-07-25
categories: CV
toc: true
---
[[논문]](https://arxiv.org/pdf/2105.01601.pdf)

## 소개
최근 CNN을 Transformer 기반의 구조로 대체하려는 노력이 이어지고 있는데요. 오늘 소개하고자 하는 논문은 CNN/Transformer 중 어떤 구조도 사용하지 않고, 오직 MLP(Multi Layer Perceptron)로만 구성된 모델인 **'MLP-Mixer'** 를 제시하였습니다.

### MLP와 CNN
흔히 CNN이 MLP보다 이미지 처리에 있어서 더 우월하다고 알려져 있습니다. 이는 CNN이 이미지 데이터의 특징인 locality를 잘 반영할 수 있으며, translation이나 rotation에 덜 민감하기 때문입니다.  
하지만 조금만 생각해보면 CNN의 모든 연산은 MLP로 구현 가능합니다. 즉, CNN 모델이 표현할 수 있는 모든 특징들을 MLP 모델로도 표현할 수 있는 것이죠. 그렇다면 마치 ResNet이 'Residual connection으로 연결하면, 깊은 모델이 적어도 얕은 모델만큼은 잘돼야 하는 것 아니야?' 라고 주장한 것처럼, 좋은 학습 방법론과 좋은 모델 구조가 있다면 MLP 모델이 CNN 모델의 성능을 압도하는 것도 가능하지 않을까요?   
... 까지가 제 생각입니다. 오늘 소개할 논문은 단순히 논문의 실험 결과와 모델의 구조에 집중하기 보다는, inductive bias를 줄여 더욱 강력한 모델을 만들려고 하는 흐름 중 하나로 이해하고 읽으시면 편할 것 같습니다!

## 모델 아키텍쳐
![mlp_mixer](/assets/images/210725/MLP-mixer.png)
아이디어 자체가 간단하므로, 긴 설명없이 바로 모델 구조를 소개하겠습니다. 기본적인 input/output 구성은 [ViT](https://dhdroid.github.io/cv/2020/12/01/ViT.html)와 동일하기 때문에 그림만으로도 크게 어렵지 않게 이해할 수 있습니다. 하나의 Mixer Layer의 입력부터 출력까지의 과정은 다음과 같이 요약할 수 있습니다.  

1. patch embedding을 input으로 받아 token-mixing MLP를 통과시켜 spatial한 차원을 mix해준다.
2. 이번엔 channel-mixing MLP를 통과시켜 각 token내의 feature들을 mix해준다.

MLP로만 구성된 대단히 단순하고, 직관적인 모델인데요. 모델의 특징적인 디테일 몇가지를 살펴보겠습니다!

### 1. channel/token mixing MLP의 parameter sharing
channel-mixing MLP는 token-invariant한 특성을 지키기 위해 parameter sharing을 합니다. 그런데, 놀랍게도 token-mixing MLP 역시 모든 channel에 대해 parameter sharing을 합니다. 이는 실험에서 parameter sharing 모델과 non-sharing 모델이 거의 똑같은 성능을 냈기 때문이라고 합니다.   
(의문인 것이, parameter가 share되는 token mixing 이라면, [Synthesizer](https://arxiv.org/pdf/2005.00743.pdf)에 언급된 global attention에 attention bias를 주는 것과 무엇이 다른지 잘 모르겠습니다.)

### 2. [Depthwise Separable Convolution](https://dhdroid.github.io/cv/2021/01/17/MobileNet.html)과 유사한 구조?
token-mixing MLP를 depthwise convolution으로, channel-mixing MLP를 pointwise convolution으로 보면 depthwise separable convolution과 굉장히 유사한 구조입니다. 두 모델 모두 feature들을 (1) 주어진 spatial location 내에서 섞고, (2) 다른 location과 섞는 과정을 분리해서 모델링했다는 것에 공통점이 있습니다. 하지만, Depthwise convolution은 (1)과정에서 각 channel 별로 다 다른 weight를 사용하지만, MLP-Mixer는 모든 channel들이 weight를 share한다는 차이점이 있습니다.

### 3. Computational Cost?
ViT와 다르게 MLP-Mixer는 **이미지의 patch수에 선형적**인 computational cost를 가집니다.  모델 구조를 나타낸 그림에서 찾을 수 있듯이, MLP block은 2개의 FC layer와 normalization layer로 구성돼 있습니다. 이때, 각 MLP block 내의 hidden feature수를 고정시킨다면, input feature 수에 선형적인 computational cost로 MLP 연산을 수행할 수 있습니다.   
간단히 생각해봅시다. N차원 feature를 N차원으로 mapping하는 FC layer는 NxN = N^2의 cost를 요구하지만, N차원을 k차원으로 mapping한 후, 다시 N차원으로 mapping하는 block은 Nxk + kxN = 2kN의 cost를 요구합니다. 이때, k를 충분히 작은 수로 설정한다면 N에 선형인 시간으로 연산을 수행할 수 있습니다. ResNet에서 BottleNeck 구조로 cost를 줄인 것과 비슷한 원리로 이해하시면 될 것 같습니다.

### 4. Positional Embedding?
token-mixing MLP가 이미 permutation에 따라 다른 output을 내보내는 position-aware한 layer이므로 Positional Enmbedding이 따로 필요하지 않습니다.

## 실험 결과

### SOTA 모델들과의 비교
![comparison](/assets/images/210725/comparison.png)
ImageNet-21k와 JFT-300M에 pretrained한 뒤, 5가지의 downstream task(ImageNet, CIFAR-10, CIFAR-100, Pets, Flowers)에서 평가한 결과입니다. ViT는 물론, HaloNet(3x3 convolution을 self-attention으로 대체), BiT(ResNet에 transfer learning)에도 크게 뒤지지 않는 성능을 기록한 모습입니다.

### Overall Performance
![performance](/assets/images/210725/performance.png)
다양한 Mixer configuarion에 따른 performance 지표입니다. ImageNet에만 train했을 때는 76.44%의 아쉬운 accuracy를 기록했지만, 데이터셋이 커질 때의 성능향상 폭이 매우 큰 것을 확인할 수 있습니다.

### Transfer Quality
![transfer](/assets/images/210725/transfer.png)
pretraining data size(가로 축)가 커짐에 따라 linear 5-shot ImageNet accuracy(세로 축)가 드라마틱하게 좋아지는 모습입니다. BiT < ViT < Mixer 순으로 data size의 중요도가 큰 것을 확인할 수 있습니다. 또한, 충분히 많은 데이터가 있을 경우에는 Mixer와 ViT의 성능이 BiT를 크게 웃도는 모습입니다.

### Permutation-Invariance
![invariance](/assets/images/210725/permutation-invariance.png)
input patch의 순서와 patch내의 pixel 순서를 섞어도 Mixer의 transfer quality의 차이가 나지 않는 모습입니다. 반면 ResNet은 큰 inductive bias때문에 조금만 shuffle을 해도 크게 성능이 떨어지는 것을 확인할 수 있습니다.

### Hidden Units of token-mixing MLPs
![hidden_units](/assets/images/210725/hidden-units.png)
각각 첫번째, 두번째, 세번째 Mixer Layer의 token-mixing MLP weight를 visualize한 모습입니다. 얕은 층에서는 CNN과 같이 선이나 무늬 등을 감지하는 weight가 나타나고, 서로 대응되는 weight의 쌍이 생기는 것을 확인할 수 있습니다.

## 결론
- MLP로만 구성된 MLP-Mixer는 최신 SOTA 모델에 근접한 performance를 달성했다.
- computational cost - accuracy tradeoff 에서는 오히려 앞서가기도 했다.
- 앞으로 CNN-Transformer-MLP를 비교하는 연구가 기대된다.

## 생각
개인적으로 모델의 Inductive bias를 최소화하려는 트렌드를 매우 긍정적으로 생각합니다. 인간이 만들어낸 bias 없이 데이터로부터 지식을 추출하는 모델을 개발하는 것이 딥러닝의 이상(?)에 더 가까워지는 방향인 것 같습니다! 딥러닝을 연구하는 사람으로서 흐뭇해지는 논문이네요.  
다만 Transformer와 MLP의 대결의 승자는 누가될 지 궁금해집니다! Transformer는 각 요소들을 weighted sum해야 한다는 약간의 bias가 존재하지만, MLP에 비해 더 adaptive한 구조를 띄고 있기 때문에 각각의 장단이 있는 것 같습니다. 앞으로의 연구가 기대되네요!